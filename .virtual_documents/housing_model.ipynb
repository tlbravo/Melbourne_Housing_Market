import pandas as pd
import numpy as np
import sklearn


housing = pd.read_csv('Melbourne_housing.csv')
housing.columns


housing['Price'].isna().sum()


#housing.dropna(inplace=True)
housing.describe()


print(housing.shape)


import matplotlib.pyplot as plt

plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=5)
plt.rc('ytick', labelsize=10)

# plots histograms of numerical data
housing.hist(bins=50, figsize=(12, 8))
plt.show()


from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)


train_set.head()


test_set.head()


housing.plot(kind="scatter", x="Longtitude", y="Lattitude", grid=True, alpha=0.2)
plt.show()


# radius(option s)- population
# color(option c)- price
housing.plot(kind="scatter", x="Longtitude", y="Lattitude", grid=True,
             c="Price", cmap="jet", colorbar=True,
             legend=True, sharex=False, figsize=(10, 7))
plt.show()


numerical_columns = ["Rooms", "Price", "Distance", "Bedroom2", 
                     "Bathroom", "Car", "Landsize", "BuildingArea", "Lattitude", "Longtitude"]
corr_matrix = housing[numerical_columns].corr()


corr_matrix["Price"].sort_values(ascending=False)


# creates a Series of True/False values if there is a null value in a column
null_rows_idx = housing.isnull().any(axis=1)

# selecting only the rows with null values
housing.loc[null_rows_idx].head()


from sklearn.impute import SimpleImputer

# instantiate the class (creates a SimpleImputer instance)
imputer = SimpleImputer(strategy="median")


housing_num = housing.select_dtypes(include=[np.number])


imputer.fit(housing_num)


housing_num = housing.select_dtypes(include=[np.number])


housing_num.median()#.values


X = imputer.transform(housing_num)


imputer.feature_names_in_


housing_tr = pd.DataFrame(X, columns=housing_num.columns, # or use above imputer.feature_names_in_
                          index=housing_num.index)


housing_tr.loc[null_rows_idx].head()


housing_cat = housing[["Regionname"]]
housing_cat.value_counts()


from sklearn.preprocessing import OneHotEncoder

# instantiate object
cat_encoder = OneHotEncoder(drop='first')

# fit & transform housing_cat
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)


housing_cat_1hot


housing_cat_1hot.toarray()


cat_encoder.categories_


cat_encoder.feature_names_in_


cat_encoder.get_feature_names_out()


print(housing_cat_1hot.shape)


df_cat = pd.DataFrame(
    housing_cat_1hot.toarray(),  # Convert sparse matrix to dense if necessary
    columns=cat_encoder.get_feature_names_out(),
    index=housing_cat.index
)

df_cat


from sklearn.preprocessing import StandardScaler

# instantiate scaler
std_scaler = StandardScaler()

# call fit_transform
housing_num_std_scaled = std_scaler.fit_transform(housing_num)

# look at first row of array
housing_num_std_scaled[0]















